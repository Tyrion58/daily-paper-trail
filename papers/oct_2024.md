# Daily Paper Trail, October 2024

OK. Let's see what I am focusing on in recent days  ðŸ¤©

## Week 2
Date: Friday, Oct 11, 2024
- **Paper 1**: [Exposing Attention Glitches with Flip-Flop Language Modeling](https://proceedings.neurips.cc/paper_files/paper/2023/file/510ad3018bbdc5b6e3b10646e2e35771-Paper-Conference.pdf)
  - **Summary**: A benchmark for a minimalistic long-range dependency. 
  - **Key Takeaways**: They proposed the flip-flop language modeling (FFLM), a parametric family of synthetic benchmarks designed to probe the extrapolative behavior of neural language models. The author thinks that this kind of capabilities is important
for chains of reasoning for LLMs.

- **Paper 2**: [Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models through Intervention without Tuning](https://arxiv.org/pdf/2312.17484)
  - **Summary**: This paper proposes a new ITI-like to do truthfulness control. 
  - **Key Takeaways**: What they do:
     - Randomly cut the answer into different length according to normalization distribution. Because they think the lase token doesn't always reflect the correctness of a reply.
     - They trained multiple probing in each attention heads. During inference, they summarize all of them to do intervention.
  - **Personal Thought**: More smart distribution to cut; The possibility of unsupervised learning.
