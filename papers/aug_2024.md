# Daily Paper Trail, August 2024
Welcome to August 2024! ðŸš€

## Week 1
Date: Aug 17, 2024
- **Paper 1**: [LOFIT: Localized Fine-tuning on LLM Representations](https://arxiv.org/pdf/2406.01563)
  - **Summary**: This paper proposes a new fine-tuning method: LoFiT. It is a method that localized fine-tune some selected attention heads by adding offset vectors to activations of these heads. 
  - **Key Takeaways**: LoFiT is a special method that uses the ideas of Representation Engineering and parameter-efficient fine-tuning (PEFT). Actually, several representation intervention methods in literature can be
cast in this framework, such as ITI. 
  - **Personal Thoughts**: Just a question: in their framework, there are 2 steps: 1) select heads to fine-tune, 2) fine-tune these heads. But both of these steps are done by training on the same dataset. But, Is the second step necessary? What if we just modify the activations of these heads using these scaling factor vectors $A_l^i$