# Daily Paper Trail, August 2024
Welcome to August 2024! ðŸš€

## Week 1
Date: Saturday, Aug 17, 2024
- **Paper 1**: [LOFIT: Localized Fine-tuning on LLM Representations](https://arxiv.org/pdf/2406.01563)
  - **Summary**: This paper proposes a new fine-tuning method: LoFiT. It is a method that localized fine-tune some selected attention heads by adding offset vectors to activations of these heads. 
  - **Key Takeaways**: LoFiT is a special method that uses the ideas of Representation Engineering and parameter-efficient fine-tuning (PEFT). Actually, several representation intervention methods in literature can be
cast in this framework, such as ITI. 
  - **Personal Thoughts**:
    - Just a question: in their framework, there are 2 steps: 1) select heads to fine-tune, 2) fine-tune these heads. But both of these steps are done by training on the same dataset. But, Is the second step necessary? What if we just modify the activations of these heads using these scaling factor vectors $A_l^i$
    - If we want to optimize this work, we may have to start with two stages of training. Is it possible to not need to learn in one of the steps?

Date: Sunday, Aug 18, 2024
- **Paper 1**: [RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation ofPre-trained Language Model for Knowledge Editing andFine-tuning](https://arxiv.org/pdf/2406.10777)
  - **Summary**: This paper proposes a new PEFT(parameter-efficient fine-tuning) method. Their baseline is LoRA. The matrices used by LoRA are often dense. It means that it won't work when internal knowledgment need to be edited. RoseLoRA's matrix is a sparse matrix. Therefore, during inference, only a small part of parameters are updated.
  - **Key Takeaways**: The opimization problem ($l_0$ problem) is NP-hard so they present how to solve the optimization problem. They prune each
row of A and each column of B based on sensitivity iteratively.
