# Daily Paper Trail, August 2024
Welcome to August 2024! ðŸš€

## Week 1
Date: Saturday, Aug 17, 2024
- **Paper 1**: [LOFIT: Localized Fine-tuning on LLM Representations](https://arxiv.org/pdf/2406.01563)
  - **Summary**: This paper proposes a new fine-tuning method: LoFiT. It is a method that localized fine-tune some selected attention heads by adding offset vectors to activations of these heads. 
  - **Key Takeaways**: LoFiT is a special method that uses the ideas of Representation Engineering and parameter-efficient fine-tuning (PEFT). Actually, several representation intervention methods in literature can be
cast in this framework, such as ITI. 
  - **Personal Thoughts**:
    - Just a question: in their framework, there are 2 steps: 1) select heads to fine-tune, 2) fine-tune these heads. But both of these steps are done by training on the same dataset. But, Is the second step necessary? What if we just modify the activations of these heads using these scaling factor vectors $A_l^i$
    - If we want to optimize this work, we may have to start with two stages of training. Is it possible to not need to learn in one of the steps?

Date: Sunday, Aug 18, 2024
- **Paper 2**: [RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation ofPre-trained Language Model for Knowledge Editing andFine-tuning](https://arxiv.org/pdf/2406.10777)
  - **Summary**: This paper proposes a new PEFT(parameter-efficient fine-tuning) method. Their baseline is LoRA. The matrices used by LoRA are often dense. It means that it won't work when internal knowledgment need to be edited. RoseLoRA's matrix is a sparse matrix. Therefore, during inference, only a small part of parameters are updated.
  - **Key Takeaways**: The opimization problem ($l_0$ problem) is NP-hard so they present how to solve the optimization problem. They prune each
row of A and each column of B based on sensitivity iteratively.
  - **Personal Thought**: From the perspective of representation engineering, can we also use this sparsity to tell stories?


## Week 2
Date: Monday, Aug 19, 2024
- **Paper 1**: [Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models](https://arxiv.org/pdf/2401.06102)
  - **Summary**: In this work, they want to "translating" the representations in hidden layers of LLMs into human-like text by using another LLM. The main idea of â€‹â€‹this paper is to imagine the large language model as a tool that can translate representations that are incomprehensible to humans into natural language.
  - **Key Takeaways**: In general, this article describes previous work using a unified architecture and conducts some of its own experiments based on this architecture. However, this work does not give a positive answer to whether the representation of different models can perform such operations.
  - **Personal Thoughts**:
 

Date: Monday, Aug 23, 2024
- **Paper 1**: [Representation Surgery: Theory and Practice of Affine Steering](https://arxiv.org/pdf/2402.09631)
  - **Summary**: In this work they borrowed the idea of â€‹â€‹concept erasure. They propose 2 steering methods: mean matching and cross-covariance matching. Under the guardedness assumption, they provide optimal affine steering functions.
  - **Key Takeaways**: In concept erasures, representations are said to be (affinely) guarded with respect to a concept if no linear classifier can recover the concept from the representations above chance. In math, it means that The functions that most degrades the performance of the classifier(Definition 3.1). Using Theorem3.1, we can convert this problem into a optimization problem. For steering models, they construct the same optimization problem but over different affine functions. At this point, the radiation function only acts on concepts that we do not want the model to have.So essentially, the model can learn the gap between the two concepts, which allows us to use such affine transformations to manipulate.
  - **Personal Thoughts**: Wonderful Idea!ðŸ’¡ðŸ¤© Maybe we can use this framework to unify other methods to do represnentation engineering.
